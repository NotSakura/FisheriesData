---
title: "Analysing the Fish Hunting Numbers in North Pacific Ocean"
subtitle: "Using Bayesian Modeling, to find that number of fish caught has increased"
author: "Shreya Sakura Noskor"
thanks: "Code and data are available at: [https://github.com/NotSakura/FisheriesData.git](https://github.com/NotSakura/FisheriesData.git)."
date: today
date-format: long
abstract: "We analyzed data on fish catches in the North Pacific Ocean using Bayesian modeling. Our analysis shows that the number of fish caught has increased over time. This suggests that fishing activities in the region have intensified. Understanding this trend is important for managing fish populations and ensuring the sustainability of the ocean's resources. This is important as the Pacific is full of different kind of fish like, trout and salmon that are essential in the food chain, but are being overhunted for food and recreational purposes."
format:
  pdf:
    bibliography: references.bib
    documentclass: article
    geometry: margin = 1in
header-includes:
  - \usepackage{float} 
  - \floatplacement{table}{H}
number-sections: true
include-in-header: 
  text:
    \renewcommand{\abstractname}{Abstract}
bibliography: references.bib
toc: true
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(janitor)
library(readxl)
library(dplyr)
library(lubridate)
library(broom)
library(modelsummary)
library(rstanarm)
library(knitr)
library(openxlsx)
```



```{r}
#| echo: false
#| warning: false
#| message: false
data <- read.xlsx('../data/01-raw_data/NPAFC_Catch_Stat-1925-2023.xlsx', sheet = 1) 
colnames(data) <- data[1, ]
data <- data[-1, ]
```


```{r}
#| echo: false
#| warning: false
#| message: false
cleaned_pac2 <- data %>%
  filter(`Data Type` == "Number (000's)", `Whole Country/Province/State` == "Whole country", Species == "Total")  %>%
  select(Country, Species, `Catch Type`, `2022`) %>%
  filter(!is.na(`2022`) & `2022` != 0)

cleaned_pac2 <-cleaned_pac2 %>%
  select(-Species)


cleaned_pac <- data %>%
  filter(Species == "Total", `Data Type` == "Number (000's)", `Reporting Area` == "Whole country") %>%
  filter(!is.na(Country)) %>%
  filter(!is.na(`1925`) & `1925` != 0) %>%
  filter(!is.na(`1946`) & `1946` != 0)


koreadata <- data %>%
  filter(Species == "Total", `Data Type` == "Number (000's)", `Reporting Area` == "Whole country", Country == "Korea") %>%
  filter(!is.na(Country)) 

combined_data <- bind_rows(cleaned_pac, koreadata)

```



```{r}
#| echo: false
#| warning: false
#| message: false

combined_data <- combined_data %>%
  mutate(across(`1925`:`2023`, as.numeric))
koreadata <- koreadata %>%
  mutate(across(`1925`:`2023`, as.numeric))

combined_data <- combined_data  %>%
  select(-`Whole Country/Province/State`, -`Data Type`, -`Reporting Area`, -Species, -`Catch Type`)
koreadata <- koreadata  %>%
  select(-`Whole Country/Province/State`, -`Data Type`, -`Reporting Area`, -Species, -`Catch Type`)

long_data_combined <- combined_data %>%
  pivot_longer(
    cols = starts_with("19") | starts_with("20"),  # Select columns starting with "19" or "20"
    names_to = "Year",
    values_to = "Catch"
  ) %>%
  mutate(Year = as.numeric(Year))

long_data_korea <- koreadata %>%
  pivot_longer(
    cols = starts_with("19") | starts_with("20"),  # Select columns starting with "19" or "20"
    names_to = "Year",
    values_to = "Catch"
  ) %>%
  mutate(Year = as.numeric(Year))
```


# Introduction

Estimand is the number of fishes that were caught by each country, for every year. Or more specifically what the rate was and if they are more likely to be fished for commercial purposes. 

# Data {#sec-data}


## Overview


The data was downloaded from @npafc_statistics and was cleaned using R [@citeR]. The data was read using @openexcel and 
@readxl, while the data was cleaned using @tidy, @tidy, @janitor, @dplyr, @knitr. The data was modeled using @modelsummary, @broom, @rstanarm, and @brms. 

To download the data go to [NPFAC's official data portal](https://www.npafc.org/statistics/) and look for "NPAFC Catch Statistics (updated 28 June 2024)". Click on that link to get the csv file containing all the data. 



## Methodology and Measurement
NPAFC has this data to download from their website. The way they gathered this data was that they are an inter-government organisation so they have access to government data based on how much fish were hunted in the respective countries. The countries in the data include Canada, Russia, Korea, Japan and Unites States of America. The way each of these countries measured this data was that when fish are being caught on international waters and report it to each other. This is strictly enforced, especially after the fall of salmon and trout population in the Pacific, majorly due to environment purposes. 

This paper look at multiple variables. We will go through them one by one: 
- First, variable we look at is Country which are either "Canada", "Russia", "Korea", "Japan", and "United States", all representing the countries that are members of this organization. This data was left unchanged.
- Next variables we look at is "Whole Country/Province/State", where the instances of these variables may either be Whole country, or the different states or provinces that was fishing and gathered that data. So for example, if the value was British Columbia then the corresponding number of fishes caught reported is the number of fish caught by the province. Throughout our data we filtered for the "Whole country" value, assuming that the numbers in each province and/or state would add up to the number in "Whole country" (which it did). This was because we were more interested in comparing the fishing trends between countries rather then within a country. 
- The next variable is "Reporting Area" which accounts for where the fishes were caught. This was also filtered by "Whole country" due to the previous reasoning. The next variable that we filtered was "Species" which contained "Cherry", "Chinook", "Chum", "Coho", "Pink", "Sockeye", "Steelhead" and "Total". These are all types of salmon except for Steelhead which is a trout and "Total" which represents all the fishes that were hunted. Although there is a lot of interesting information to uncover if we did a deeper analysis on each fish, but, we decided that the best way to compare the fishing trends between countries would be to just look at the total fishes caught. 
- The next variable we used and actually analyse is the "catch type". This tells us whether is the fishes were caught for commercial purposes (caught for profit purposes like selling), sporting purposes (which means they were caught recreationaly) or subsistence purposes (which means they were caught to provide food, not as a profit). 
- The last variable we filtered was "Data Type" which was the unit that these numbers were reported in. There was Numbers in 1000s or Round weight in metric tonne. We chose to filter with numbers in 1000s as the other option was done only by the US, who provided both units.

These were the variables that we filtered but the variable we actually analyse is the number of fishes caught. That number in the raw data was provided as the year as a column and the corresponding value as the number of fishes caught. This format meant that when we are creating models or graphs it is very difficult to work with it. And so we actually shifted, rather pivotted the table so that analysis is easier. To pivot the table we created 2 new rows to the dataset, "Year" and "Catch". The year corresponds to the column's title which is the year this data is for and the "catch" refers to the number of fishes that were caught in that year, in that country. This helped significantly with making the models and such. 

## Data Visualization {#sec-graphs}

we make the assumption that the columns that say whole country it also include the provinces and different areas number as well. 

```{r, fig.height= 3.5, fig.width= 7.5}
#| echo: false
#| warning: false
#| message: false
#| label: fig-long_comb
#| fig-cap: "Catch over Time by Country"

ggplot(long_data_combined, aes(x = Year, y = Catch, color = Country)) +
  geom_line(size = 1, alpha = 0.7) +  # Line plot with some transparency
  geom_point(size = 2, alpha = 0.8) +  # Points with smaller size
  labs(x = "Year", y = "Catch") +
  scale_y_continuous(labels = scales::comma) +  # Format y-axis with commas
  theme_minimal() +
  theme(legend.title = element_blank())

```
From @fig-long_comb we are able to see the number of fishes caught in each country as time goes by. They fluctuate a lot because over fishing in one year means that the next year there is not a lot of fish left as not as many survived to repopulate. However we see that there is a increase in the number of fishes caught for the US and Russia. Meanwhile, Japan and Canada are declining. This is interesting as Japan is surrounded by the Pacific Ocean more than all of these other countries and Canada coming in a close second. An educated guess as to why this may be the case is that, the US and Russia may be fishing more as they have a stronger fishing industry with the technology and money to fund longer trips in the ocean. 


```{r, fig.height= 3.5, fig.width= 6.5}
#| echo: false
#| warning: false
#| message: false
#| label: fig-korea
#| fig-cap: "Catch over Time by Country"

ggplot(long_data_korea, aes(x = Year, y = Catch, color = Country)) +
  geom_line(size = 1, alpha = 0.7) +  # Line plot with some transparency
  geom_point(size = 2, alpha = 0.8) +  # Points with smaller size
  labs( x = "Year", y = "Catch") +
  scale_y_continuous(labels = scales::comma) +  # Format y-axis with commas
  theme_minimal() +
  theme(legend.title = element_blank())

```

In @fig-long_comb we can barely see the fluctuation for Korea; it seems almost constant. Hense we graph it sepreatly here in @fig-korea. We see that like the other countries in our data, they also fluctuate in numbers. From this an guess may be that they are decreasing the number of fishes they catch but, our guess is quite the opposite. We think that in 2023 it was just low due to one of the fluctuations where they fished too much the previous year. We expect a rise in the next 2 or 3 years. 

```{r, fig.height= 3.5, fig.width= 7.5}
#| echo: false
#| warning: false
#| message: false
#| label: fig-total_bar
#| fig-cap: "Catch Data by Country and Catch Type (2022)"

# Load necessary libraries
library(dplyr)
library(ggplot2)
library(tidyr)

# Create a complete grid of all possible combinations of `Country` and `Catch Type`
complete_data <- expand.grid(
  `Catch Type` = unique(cleaned_pac2$`Catch Type`),
  Country = unique(cleaned_pac2$Country)
)

aggregated_data <- cleaned_pac2 %>%
  group_by(`Catch Type`, Country) %>%
  summarise(
    total_catch = sum(as.numeric(`2022`), na.rm = TRUE), # Convert to numeric
    .groups = "drop"
  )


# Replace "United States" with "US" in the Country variable
full_data <- complete_data %>%
  left_join(aggregated_data, by = c("Catch Type", "Country")) %>%
  mutate(total_catch = ifelse(is.na(total_catch), 0, total_catch)) %>%  # Replace NA with 0 for missing catch data
  mutate(Country = ifelse(Country == "United States", "US", Country))  # Abbreviate United States to US

# Create the plot with the aggregated data and numbers on the bars
ggplot(full_data, aes(x = `Catch Type`, y = total_catch, fill = Country)) +
  geom_bar(stat = "identity", position = "dodge") +  # Bar plot with grouped bars by country
  geom_text(aes(label = paste(Country, ":\n", round(total_catch, 0))),  # Newline between Country and catch number
            position = position_dodge(width = 0.9), 
            vjust = -0.1,  # Adjust vertical position of text
            size = 2) +  # Smaller font size for the numbers
  labs( 
       x = "Catch Type", 
       y = "Catch Data") +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal() +
  theme(legend.title = element_blank())

```
@fig-total_bar is a great way to analyse the different reasons why these fishes are caught. We narrowed the year to 2022 as we predict the fishing industry has recovered after covid and will show accurate results. Here we see that most fishes are caught due to commercial purposes where they sell the fish rather then for sporting or subsistence purposes. It also seems that Russia fishes the most out of the Norther Pacific region with its number being almost doubled from the second most country to fish, the US. Russia has fished around 309,142,000 fishes in the year 2022 next to US who has fished around 166,114,000. With these numbers, no wonder we see fluctuation in the number of fishes caught in @fig-long_comb.




\newpage 
# Model
Here we model the data in 2 ways. The first way is to model the rate at which the fishes are being caught for any country. And the second model looks at what is the probability that a country is fishing for commercial purposes. 

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`. We also use `brm` package from @brms

## Number of Fishes Caught (dependent on Year and Country) {#sec-mod1}

Define $Catch_{ij}$ to be the number of fishes caught. We are trying to see how will $Catch_{ij}$ change as we increase $Year$ which is our other variable. We are finding the correlation with the countries in mind which is why we account for the random effect of the country. 


\begin{align} 
\mbox{Catch}_{ij} &\sim \mbox{Normal}(\mu_{ij}, \sigma^2) \\
\mu_{ij} &= \beta_0 + \beta_1 \times \mbox{Year}_{ij} + u_j \\
u_j &\sim \mbox{Normal}(0, \sigma_{\mbox{Country}}^2) \\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\end{align}

### Model Justification
- $Catch_{ij}$ is the number of fishes caught, the variable we are modeling
- $Year$ is the independent variable that shows the year
- $u_j$ is the random effect of the countries. This is important as the number of fishes caught differentiate between country so to get a weighted result we add this variable. 
- $\beta_0$ and $\beta_1$ tells us the intercept and the slope respectively, of the Bayesian Generalised Linear Mixed Model. 

We predict that even thought 2 (may be 3) out of the 5 countries in the data set have a positive trend in terms of the number of fish being caught (@fig-long_comb), because the rate at which Russia and USA is increasing is way higher than Japan, Korea and Canada. This can be seen visually. So I think that the rate of increase is going to be positive. 



```{r}
#| echo: false
#| warning: false
#| message: false
library(tidyr)
library(rstanarm)

#model_formula <- Catch ~ Year + (1 | Country)

#bayesian_model <- stan_glmer(
#  formula = model_formula,
#  data = long_data_combined,
#  family = gaussian(),  # Adjust this if using a different distribution for your data
#  prior = normal(0, 2.5, autoscale = TRUE),
#  prior_intercept = normal(0, 2.5, autoscale = TRUE),
#  seed = 123,
#  adapt_delta = 0.95,
 # cores = 4
#)
#saveRDS(
#  bayesian_model,
#  file = "../models/bayesian_model1.rds"
#)
bayesian_model <-
  readRDS(file = here::here("models/bayesian_model1.rds"))
```






## To be Commercial or Not to be Commercial {#sec-mod2}
Define $IsCommercial_{ij}$ to be the probability that the fishes caught were commercial (for buisness purposes). We are trying to see how will $IsCommercial_{ij}$ change as we change $Country$. We esstially use, again, a Bayesian Generalised Linear Mixture Model to see how does the likelihood of fishes being caught for commercial purposes varies from country to country.  

\begin{align} 
\mbox{IsCommercial}_{ij} &\sim \mbox{Bernoulli}(p_{ij}) \\
\mbox{logit}(p_{ij}) &= \beta_0 + u_j \\ 
\mbox{u}_j &\sim \mbox{Normal}(0, \sigma_{\mbox{Country}}^2) \\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\ 
\sigma_{\mbox{Country}} &\sim \mbox{Normal}(0, 2.5)
\end{align}

### Model Justification
- $IsCommercial_{ij}$ is the probability that the fishes caught were commercial. This is what we are modelling using Bernoulli Distribution. 
- $Country$ is the independent variable that shows the countries. 
- $u_j$ is the random effect of the countries. This is important as the number of fishes caught commercially is different between countries. 
- $\beta_0$  tells us the intercept so, what is the likelihood of it being commercial for a "baseline" country. This is essentially the average of all the intercepts for all the countries.  
Note that this model doesn't look at the number of fishes caught rather the number of occurrences in the year 2022 where fishes were caught commercially. We focus on the year 2022 because it is recent data and modeling with all years between 1925 to 2023 would have not given us a result that was relevant to us. We would also loose information as the liklihood may have not been as high in the 1900s due to the lack of equipment. 

We predict that the outcome or rather the probability that the fishes caught were commercially would be positive. This is because as we were looking at the graphs in the data section, we notice that @fig-total_bar shows most of the fishes caught were commercially and then as a sport and then as a subsistence. 

```{r}
#| echo: false
#| warning: false
#| message: false

library(brms)

# Specify priors
#priors <- c(
#  set_prior("normal(0, 2.5)", class = "Intercept"),   # Prior for the intercept
#  set_prior("normal(0, 2.5)", class = "sd")           # Prior for the standard deviation of random effects (Country)
#)

#cleaned_pac2$Is_Commercial <- ifelse(cleaned_pac2$`Catch Type` == "Commercial", 1, 0)

# Fit the model using brms
#bayesian_model_commercial_brms <- brm(
#  formula = Is_Commercial ~ (1 | Country),  # Random effect for Country
#  data = cleaned_pac2,
#  family = bernoulli(),                    # Bernoulli family for binary outcomes
#  prior = priors,                          # Use the prior argument here
 # seed = 123
#)

#saveRDS(
#  bayesian_model_commercial_brms,
#  file = "../models/bayesian_model2.rds"
#)

bayesian_model_commercial_brms <-
  readRDS(file = here::here("models/bayesian_model2.rds"))
# Extract random effects using brms
ranef_bayesian_model_commercial_brms <- ranef(bayesian_model_commercial_brms)
```



# Results

## Model Results

```{r}
#| echo: false
#| eval: true
#| warning: false
#| label: tbl-modelSum
#| layout-ncol: 2
#| tbl-cap: "Model Summaries"
#| tbl-subcap: ["bayesian model summary for predicting the probability of country fishing for commercial purposes.", "bayesian model summary for predicting the rate of fishes being caught"]

modelsummary::modelsummary(
  list(
    "Second model" = bayesian_model_commercial_brms
  ),
  statistic = "mad",
  fmt = 2
)

modelsummary::modelsummary(
  list(
    "First model" = bayesian_model
  ),
  statistic = "mad",
  fmt = 2
)
```

In @tbl-modelSum-1 we see the model summary for Model 1 which was finding the probability of a country fishing for commercial purposes. And in @tbl-modelSum-2 we see the results for model 2 where we look at the rate at which fish are being hunted for the "baseline" country. There are a lot of numbers here that we don't need to look at to see if our predictions were right. We only need to know the intercepts and the slopes or as shown in the model $\beta_0$ and $\beta_1$. In @tbl-modelSum-1 we see that the log likelihood of a "baseline" country fishing for commercial purpose is 0.84 which is 69.8%. This means that there is 69.8% probability that the country is fishing for commercial purposes, without even knowing the country. The "sd country Intercept" just tells you how much variation there is in that result. Next in @tbl-modelSum-2 we see that the intercept is -1,367,335 which tells you that at Year 0 somehow a baseline country fished negative -1,367,335 fish. We will discuss more about this number and interpret it in the next section (@sec-disc). The "Year = 721.74" tells you that with every increase in year there is about 721,000 more fish that are being caught. And finally similar to the other model summary "sd country $\times$ Intercept Intercept" tells you how much variation there is in the results. We don't need these variation values unless we are analysing the model (and not the data).  


## Other statistics

```{r}
#| echo: false
#| warning: false
#| message: false
#| results: hide
##| label: fig-randomInter
##| fig-cap: "Distribution of Random Intercepts for Country"
# Extract random effects for Country
random_effects_df <- as.data.frame(ranef_bayesian_model_commercial_brms$Country)

# Check the column names (they are already known: "Estimate.Intercept", etc.)
names(random_effects_df)

# Now, plot the distribution of the random intercepts for Country
#library(ggplot2)

#ggplot(random_effects_df, aes(x = Estimate.Intercept)) + 
#  geom_histogram(binwidth = 0.1, fill = "grey", color = "black", alpha = 0.7) +
#  labs(
#       x = "Random Intercept (Country)",
#       y = "Frequency") +
#  theme_minimal()


```



```{r}
#| echo: false
#| warning: false
#| message: false
#| results: hide
#| label: fig-predProb
#| fig-cap: "Predicted Probability of Commercial Catch by Country"
# Extract random effects for Country
random_effects_df <- as.data.frame(ranef(bayesian_model_commercial_brms)$Country)

# Add the country names (indexing row names)
random_effects_df$Country <- rownames(random_effects_df)

# Check the structure to confirm the data is correct
head(random_effects_df)

# Extract the fixed intercept (the global intercept)
fixed_intercept <- fixef(bayesian_model_commercial_brms)[1]  # First element corresponds to the intercept

# Add the random intercepts to the fixed intercept to get the log-odds
random_effects_df$log_odds <- fixed_intercept + random_effects_df$Estimate.Intercept

# Compute predicted probabilities using the log-odds
random_effects_df$predicted_prob <- 1 / (1 + exp(-random_effects_df$log_odds))

# Plot the predicted probabilities for Commercial catches across countries
library(ggplot2)

ggplot(random_effects_df, aes(x = Country, y = predicted_prob)) + 
  geom_bar(stat = "identity", fill = "grey", color = "black") +
  geom_text(aes(label = round(predicted_prob, 2)), vjust = -0.5) +  # Add text labels rounded to 2 decimals
  labs(
       x = "Country",
       y = "Predicted Probability") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  # Rotate country names for readability
  theme_minimal()


```

We also calculate the predicted probability of a country fishing commercially for all 5 countries in the NPFAC. In @fig-predProb we see that Canada, US and Russia has a 40% probability that they fish commercially, Japan and Korea both have 97% probability. We will see why that is the case in the next section (@sec-disc).

\newpage

# Discussion {#sec-disc}

## Story of the Data


## Weaknesses and next steps
Korea didn't report data until 1969 so they are left out of the first model in general. 

\newpage

\appendix

# Fisheries and Surveys/Observational Data {#sec-A}

The data that we gathered for this paper is definitely observational data. This is because the in the NPFAC's official website they state that they gather the data by contacting each member of government and asking them to submit this data. In our own Canadian government, we see that we have Fish Monitoring Policy where people are expected to report their catch data if they caught a lot [@govFish]. For example, if some one has fished a rare specie of fish or a large number of fishes were caught they must report it to the government. This is how the government gains the data and passes it on to NPFAC. 



The common concern with datasets of our type is often 2 paradoxes know as Simpson's Paradox and Berkson's Paradox. We assume that Simpson's paradox does come into play for our model in @sec-mod1. Simpson's Paradox is when we have relationship between subsets of data show a result but when we model the entire result together, the result differs. This can be seen in our first model because we see that in @fig-long_comb that different countries have a different rate of change for the number of catches with respect to year. We see that Japan and Canada has a decreasing rate. However the over all model outputs a positive rate of change. This shows that the subset of the data has a different result then when we combine the entire data. So this paradox does apply to us. Next we also say that the Berkson's Paradox also affects us as in our data set we only take a look at the data for Salmon and Trout Population. However, what the dataset doesn't account is the other fish species that are also fished. Yes it is true that over fishing is an issue but the result that our model shows may not be the most accurate number due to the fact that this key information is left out. The @popFish shows that salmon, tuna and trout are one of the most fished anadromous. And @npafc_species shows the other anadromous fishes that are in the North Pacific Ocean. 


We observed that one of the reasons why these paradox applies to us is because our data doesn't sample the entire data. The way they gather their data is similar to an Integrated method (mentioned in Stantcheva's supplemental appendix in @sampling). Although they are not an open community where anyone who is a member of the community may be rewarded for their participation in giving their data, all of the countries is under an organisation and they give their data as they all actively participate in the organisations goal for sustainable fishing. The issue we were talking about earlier- the one that causes these paradoxes to affect our data - is the fact that the way this organisation collects data also presents a sampling bias. A sampling bias is when the data collected is not a representitive of the entire population. This can be caused when data is collected only from one place and we ignore all other factors. This is what happens here. Because NPFAC only consists of the 5 countries in the data (Japan, Korea, Russia, US, Canada), our data has gaps for the number of fishes caught by other countries that fish in the North Pacific Ocean such as other East Asian countries like China, Phillipines and Taiwan. Realistically China has a large fishing industry as well so our analysis would look very different when we include their data. Another thing we are missing is the fact that there are other Anadromous fishes in the Pacific that could also skew the data and causes the Berkson's Paradox. 

We will now move on from critiquing the data to our analysis. Our modeling is a prime example of convenience sampling. This is when we filter data to our own rules and analyse those results because it is the most convenient for us. In this paper it was done for @fig-long_comb. This was due to the fact that not a lot of the data dated back to the 1925 so, initially Korea was left out from our data. However, because we wanted a chance to compare all 5 countries of the organisation, we combined data from Korea which started 1969. We purposefully chose filters that will shorten our data while getting getting the attributes we want to study. The effect of this is that our model where we check the rate of change of fishes caught [@sec-mod1], is inaccurate as we don't have the data from 1925 to 1969 for Korea and we filtered the data so much to fit Korea that we loose information like the type of fish and why they were fished (commercial, sporty, subsistence). 


Now we will move to the survey analysis. Because this data was not gathered through a survey we will simply talk about how we would gather this data. It would not be very complicated. Our hope is that 6-months before we send the survey we ask the fisher to keep track of their weekly fish caught. That they would record the number of fish caught of each type, the area they were in and when they caught them (time of day would be appreciated but the date is minimum). Then 6 months after the notice I send out a survey that asks for their information. It would be nothing private like the area they live in and how long they spent fishing. I would send out this email to corporations for their data as well as regular fisherman that fish for subsistence. This would be to make sure there is not sampling bias when collecting the data. This was an issue in our original data as we noticed there were a lot more commercial fishes caught and we did not know if this was due to not a lot of fish caught for subsistence or they were not reached out to for their data. This would make sure that we have as minimal gaps in our data as possible. 


# Data Sheet

*1. For what purpose was the dataset created?*
The data set was originally created by North Pacific Anadromous Fish Commission [@npafc_statistics] created this dataset to see how much fishing of trout and salmon was done in international waters. They strictly prohibits mass fishing of these highly demanded fishes and hence the dataset. We use the dataset to analyse the number of these fishes caught and see if there is a reason for NPAFC to be worried, and if there is a solution to this. 

*2. What do the instances that comprise the data set represent (for example, documents, photos, people, countries)?*
The instances of these data is either numeric or categorical. Thee first couple of columns tells you the country the data is from, where the fishes were hunted as well as getting to specific regions. Why they were hunted and what the unit were, was also in the data set. The majority of the dataset is numbers expressing how much fish was hunted, either in thousands or tonnes. 

*3. Is any information missing from individual instances?*
There are several instances of data missing. Most are from the number of fishes column as some of the data goes back to 1925 but, not all of them so, there are some rows of data where the total number of fishes collected in 1928, for example, is not present. 

*4. How was the data associated with each instance acquired? Was the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/ derived from other data (for example, part-of-speech tags, model-based guesses for age or language)?*

This dataset was reported by the subjects of each country. Meaning this organisation asked the government of Canada, US, Korea, Russia and Japan, send in the numbers and they compiled this data. 


\newpage

# Model Card - North Pacific Anadromous Fish Data
**Model Details**
- Model was created by Student at University of Toronto on November 27th 2024. 
- Bayesian Generalized Linear Model (GLMM)
- Ver 4 as the other versions tried to take all variables like region and country into consideration but the model was too complex to implement and even more to analyse. 


**Intended Use**
- Analyse the number of Anadromous fish caught in the North Pacific Ocean by looking at if they are commercially caught and the rate at which they were caught. 
- Intended to see if sustainability measures put on fish hunting has worked
- Not intended to see what measures can be put into change the outcome of the results seen here (such as laws to prevent the rate being so high)


**Factors**
- Relevant factors are year, number of fishes caught and if the fishes were caught for commercial purpose or not (true or false value). Year is a predetermined value and the countries that reported the data to NPFAC was in charge of identifying the number of fishes they caught and why they caught it. Details available in data section of the paper. 
- Evaluation factors or the values that are being reported are rate of number of fish caught [@fig-modelcard-2] and probability that the fish caught was commercial [@fig-modelcard-3]. These are being evaluated to understand if certain sustainability measures are helping the fish being over hunted.  


**Metrics**
- Measured in Thousands of Fish as given in @npafc_statistics.


**Training Data**
- Data from @npafc_statistics was used to make/train this model. 


**Evaluation Data**
- We applied the model on the same data as the training set [@fig-modelcard-2] and we see that it is linear due to the fact that we applied the model on it self with the same rate. 


**Ethical Considerations**
- We are only using the observational data. Hence we are not actually changing the data themselves. The model does not take into account any personal data as the only data provided was the year, the number of fish caught and where they were caught. 


**Caveats and Recommendation**
- Use a data that has no sampling bias. This is because of the reasons discussed in @sec-A. To summaries it, it is because of the Simpson's and Berkson's Paradox that seems to arise. 


```{r}
#| echo: false
#| eval: true
#| warning: false
#| label: fig-modelcard
#| layout-ncol: 3
#| fig-cap: "Quantitative Analysis"
#| fig-subcap: ["Bar plot with error bars for predictions and credible intervals by Country", "Plot predictions over time (by Year) for each Country", "Predicted Probability of Commercial Catch by Country"]
#| results: hide

# Load necessary libraries
library(rstanarm)
library(ggplot2)

# Assuming the model is already fitted with stan_glmer like this:
# model_formula <- Catch ~ Year + (1 | Country)
# bayesian_model <- stan_glmer(formula = model_formula, data = long_data_combined, family = gaussian(), 
#                              prior = normal(0, 2.5, autoscale = TRUE), 
#                              prior_intercept = normal(0, 2.5, autoscale = TRUE), 
#                              seed = 123, adapt_delta = 0.95, cores = 4)

# Extract posterior predictions from the fitted model
posterior_preds <- posterior_predict(bayesian_model)

# Summarize the predictions (mean and credible intervals)
mean_preds <- apply(posterior_preds, 2, mean)  # Mean of posterior predictions for each data point
lower_ci <- apply(posterior_preds, 2, function(x) quantile(x, probs = 0.025))  # 95% lower credible interval
upper_ci <- apply(posterior_preds, 2, function(x) quantile(x, probs = 0.975))  # 95% upper credible interval

# Check the number of rows in the original data and posterior predictions
print(nrow(long_data_combined))  # Should be 495 (or the correct number of rows)
print(length(mean_preds))  # Should match the number of rows in long_data_combined

# Handle potential mismatch in number of rows (e.g., due to missing values)
# If there are any missing values, we can subset the data or remove rows with NAs
long_data_combined_clean <- long_data_combined[complete.cases(long_data_combined$Catch), ]

# Ensure the posterior predictions align with the cleaned data
# If long_data_combined_clean has a different number of rows than the posterior predictions,
# subset the predictions accordingly.
if (nrow(long_data_combined_clean) != length(mean_preds)) {
  stop("The number of posterior predictions does not match the cleaned data.")
}

# Add the predictions and credible intervals to the cleaned data
long_data_combined_clean$mean_pred <- mean_preds[1:nrow(long_data_combined_clean)]
long_data_combined_clean$lower_ci <- lower_ci[1:nrow(long_data_combined_clean)]
long_data_combined_clean$upper_ci <- upper_ci[1:nrow(long_data_combined_clean)]

# Visualizing the predictions using ggplot2
# Bar plot with error bars for predictions and credible intervals by Country
ggplot(long_data_combined_clean, aes(x = Country, y = mean_pred, fill = Country)) +
  geom_bar(stat = "identity", position = "dodge", color = "black") +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +
  labs(title = "Model Predictions by Country with 95% Credible Intervals", 
       x = "Country", 
       y = "Mean Prediction") +
  theme_minimal()

# If you want to plot predictions over time (by Year) for each Country
ggplot(long_data_combined_clean, aes(x = Year, y = mean_pred, color = Country)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ Country, scales = "free_y") +  # Facet by Country
  labs(title = "Predictions of Catch by Year and Country", x = "Year", y = "Mean Prediction") +
  theme_minimal()

# Optionally, compute and visualize R-squared or other performance metrics
# Compute R-squared
observed_values <- long_data_combined_clean$Catch
mse <- mean((observed_values - long_data_combined_clean$mean_pred)^2)  # Mean squared error
r_squared <- 1 - sum((observed_values - long_data_combined_clean$mean_pred)^2) / sum((observed_values - mean(observed_values))^2)

# Print MSE and R-squared
print(paste("MSE:", mse))
print(paste("R-squared:", r_squared))

# Optionally, you can add these metrics to the data for further visualization if needed
long_data_combined_clean$mse <- mse
long_data_combined_clean$r_squared <- r_squared

ggplot(random_effects_df, aes(x = Country, y = predicted_prob)) + 
  geom_bar(stat = "identity", fill = "grey", color = "black") +
  labs(
       x = "Country",
       y = "Predicted Probability") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  # Rotate country names for readability
  theme_minimal()
```





\newpage
# References
